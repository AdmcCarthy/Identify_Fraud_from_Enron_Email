===============================
Identify Fraud from Enron Email
===============================

*Adam McCarthy*

*Problem posed in Udacity Intro to machine learning*

Getting Started
---------------

Following the biggest corporate scandal in American history
can emails and finacial information be used to seperate and
predict persons of interest. A hunt for those that may be
related to criminal activities.

To test results:

.. code-block:: bash
    
    $ cd final_project
    $ tester.py

To re-run:

.. code-block:: bash

    $ cd final_project
    $ poi_id.py

Overview of data
----------------

Person of interest - Label to be predicted
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

The predicted label is person of interst (POI). A person of interest
reflects those in the Enron case who have been
indicted, settled without admitting guilt
or testified in exchange for immunity.

The list of POIs has been generated by Udacity.
The list was hand drafted from various
sources so could contain 
errors.

There are 35 persons of interst in total
30 of which worked for Enron.

Jeffrey Skilling was the CEO during the fraud period.

Kenneth Lay was chairman.

Andrew Fastow was CFO.

Email dataset
~~~~~~~~~~~~~

The email dataset is from ` <https://www.cs.cmu.edu/~./enron/>`_

Email dataset consists of 150 directories each reflecting a person,
specified as lastname followed by first letter of first name.

There are 86 people with email data suggesting that those
without financial data have not been used.

Within poi_names.txt it can be seen with a yes (y),
no (n) column if the poi has an email directory
in the dataset. This means the majority
of poi do not their email inboxes within the Enron Email
dataset.

Financial dataset
~~~~~~~~~~~~~~~~~

The financial information is sourced from the Enron
insiderpay pdf which is from Case No. 01-16034.

There are POIs who have email information but do not
have financial information.

Non-POIs all come from the financial spreadsheet.

Only POIs or non-POIs who have financial information are used
as combining POIs without any financial information,
i.e. they have all NaN values for financial data will
cause issues with the machine learning process.

An alternative approach would be to only use email
information to be able to exapand the POI and non-POI
dataset but that will not be taken further here.

There anomalous values in the dataset.

One person value is TOTAL, which gives a sum of
values, rather than relating to being a person.
This is removed during the data processing pipeline.

Enron Final Project dataset
~~~~~~~~~~~~~~~~~~~~~~~~~~~

The dataset created by Udacity is aggregated to contain email
and finacial information.

It is set up as a key value pair where each key is a person with
all the features stored as a dictionary as that person value.

There are 146 persons within the dataset. For each person there
are 21 variables.

The dataset contains data on 18 of the POIs.

Note that when missing values occur 
featureFormat() and targetFeatureSplit()
will replace this with 0.

Most of the values have a range of missing parameters,
see table below.

.. csv-table:: Datset Variables
   :header: "Variable, "Missing Values"
   :widths: 15, 5

    "bonus", 64
    "deferral_payments", 107
    "deferred_income", 97
    "director_fees", 129
    "email_address", 35
    "exercised_stock_options", 44
    "expenses", 51
    "from_messages", 60
    "from_poi_to_this_person", 60
    "from_this_person_to_poi", 60
    "loan_advances", 142
    "long_term_incentive", 80
    "other", 53
    "poi", 0
    "restricted_stock", 36
    "restricted_stock_deferred", 128
    "salary", 51
    "shared_receipt_with_poi", 60
    "to_messages", 60
    "total_payments", 21
    "total_stock_value", 20

The TOTAL key relates to an eroneous input, it is
a an order of magnitude larger than other values. 
It is the sum of all people in the dataset and is removed using:

Other large values have been checked and are
associated to real people. See enron61702insiderpay.pdf
for evidence.

Email Variables
---------------

From messages
~~~~~~~~~~~~~

Out of 86 people the mean number of messages from them is 609.
The range is 12 to 14368, with a median of 41. 
This suggests a highly skewed dataset.

From poi to this person
~~~~~~~~~~~~~~~~~~~~~~~

Out of 86 people the mean is 65.
The range is 0 to 528, with a median of 35.
This would be a skewed dataset.

From this person to poi
~~~~~~~~~~~~~~~~~~~~~~~
Out of 86 people the mean is 41 emails.
The range is 0 to 609, with a median of 8.
This is highly skewed.

Shared receipt with poi
~~~~~~~~~~~~~~~~~~~~~~~

Out of the 86 with email data the mean is 1176.
The range is 2 to 5521, with a median of 741.
This is skewed but the mean and median are higher.

To messages
~~~~~~~~~~~

Out the email data the mean is 2074.
The range is 57 to 15149, with a median of 1211.
This is highly skewed.

Financial variables
-------------------

Bonus
~~~~~

82 people have bonus information, with a mean of 2,374,235$.
The range is 70,000$ to 97,343,620$.

.. image:: docs\images\Top_Bonuses.png
   :scale: 100 %

Deferral payments
~~~~~~~~~~~~~~~~~

39 people have this information, with a mean value of 1,642,674$.
The range is -102,500$ to 32,083,400$.
Unsure why this could be negative.

Deferred income
~~~~~~~~~~~~~~~

49 people with a mean of -1,140,475$.
The range is -27,992,890$ to -833$.
This is a negative variable.

Director fees
~~~~~~~~~~~~~

17 people have this information, with a mean of 166,804$.
The range is 3285$ to 1,398,517$.

Exercised stock options
~~~~~~~~~~~~~~~~~~~~~~~

102 people have information, with a mean of 5,987,054$.
The range is 3285$ to 31,176,400$.

Expenses
~~~~~~~~

95 people have this information, with a mean of 108,728$.
The range is 148$ to 5,234,198$.

Loan advances
~~~~~~~~~~~~~

Only four have this information.
The mean on these four values is 41962500$.
The range is 1,600,000$ to 83,925,000$.
This is a low number of people but a very
large amount of money.

Long term incentive
~~~~~~~~~~~~~~~~~~~

66 people. Mean of 1470361$.
The range is 69223$ to 48521930$.

Other
~~~~~

93 people have this value. The mean is 919,065$
The range is 2$ to 42,667,590$.

Restricted stock
~~~~~~~~~~~~~~~~

110 have this value. The mean is 166,410$
The range is 2,604,490$ to 130,322,300$

Restricted stock deffered
~~~~~~~~~~~~~~~~~~~~~~~~~

18 have this value. The mean is 166,410$.
The range is -7,576,788$ to 15456290$.

Salary
~~~~~~

95 have salary information, the mean is 562,194$.
The range is 477$ to 26704230$.
The lowest salary seems a strange number for salary.

.. image:: docs\images\Top_Salaries.png
   :scale: 100 %

Salary can be compared to bonus as these are
two variables that may be correlated.

.. image:: docs\images\salary_bonus.png
   :scale: 100 %

The plot also splits the data into two sets
to view how a linear regression model would
behave. The data has a large spread with a
couple of key outliers. These outliers mean
that a linear model is only useful for the
cluster of values associated with lower salary
and smaller bonuses. The outliers drag the regression
model, for example see the blue trend line.

All outliers are interesting data points.
High salary, high bonus pairs are the top
paid in the company. While high bonus 
moderate salary is a question why they
have such high bonus.


Total payments
~~~~~~~~~~~~~~

125 have this value, with a mean of 5,081,526$.
The minimum is 148$ and the maximum is 309,886,600$

Total stock value
~~~~~~~~~~~~~~~~~

126 have information about total stock value.
The mean is 6,773,857$. The range is -44,093$
to 434,509,500$

Summary
~~~~~~~

Some of the figures here are astonishing. The high figures
and skewed distribution suggest a number of these datasets
are over disperssed.

Ther are also some suspicious low values like a the minimum
salary.

Outlier removal
---------------

TOTAL is removed as this is a sum of all people.

THE TRAVEL AGENCY IN THE PARK is removed as this is not a valid person.

These are removed from the dataset at the start of the data processing
pipeline.

.. code-block:: Python

    if ro:
        data_dict.pop("TOTAL", None)
        data_dict.pop("THE TRAVEL AGENCY IN THE PARK", None)

It can be turned off by setting ro to FALSE.

Feature selection
-----------------

Four ensemble or tree classifiers are run to investigate
feature importance. This is using the entire dataset
and all variable apart from email address and name of person.

The prediction is for the target, POI.

.. image:: docs\images\DT_feature_importance.png
   :scale: 100 %

.. image:: docs\images\RF_feature_importance.png
   :scale: 100 %

.. image:: docs\images\AB_feature_importance.png
   :scale: 100 %

.. image:: docs\images\GB_feature_importance.png
   :scale: 100 %

Exercised stock options is the most important
feature in three of the classifiers.

In AdaBoost the deffered income followed by bonus
are the most important.

Decision tree does not use many of the variables.

Director fees is consitently of low (almost no) importance.

Loan advances is of low importance but has minor
impact.

restricted_stock_deferred is either of no importance
or of minor importance. Similary deferral_payments is
of no to little importance.

This gives four variables with very little importance,
Director fees, loan advances, restricted stock deffered
and defferal payments.

A way to select these variables will be using
a limit on importance. For example AdaBoost feature
importance <0.02 will remove the weakest four
variables. Upon implementation a default ratio of
0.01 is used as the cut-off.

The moderate variables tend to change in importance
between the different algorithms. For example
from_poi_to_this_person. These variables may
have potential to be combined in pairs or other combinations.
This will reduce the total number of variables
and potentially increase the signifance.

Feature engineeering
--------------------

Email data
~~~~~~~~~~

Within the email data there are five variables.

.. image:: docs\images\email_poi.png
   :scale: 100 %

The bubble chart highlights all five variables by
combining two in ratios along x and y. These ratios
seem suitable candidates for feature engineering.

One takes the ratio of emails from a POI compared to
the total number of emails to that person.

The second the ratio of emails to a POI compared to
the total number of emails that person has sent.

The idea being that this will highlight persons of
interest better than the two variables seperately.

When using these ratios the input variables will
be removed. So from_messages, to_messages, from_poi_to_this_person
and from_this_person_to_poi are not used when using feature engineering.

Feature Scaling
---------------

Feature scaling is often a requirement for effective machine learning.

Exploratory data anlysis has shown that even after removing the
extreme outlier, TOTAL, a number of the variables have over
disperssed data.

A robust scaler can be used for datasets with many outliers. This will
use more robust estimates for central tendancy and dispersion before
scaling the dataset.

Cross-validation and optimization
--------------------------------- 

To make a classifier that works well on new or unseen data
cross validation aids the algorithm from overfitting on the
training data.

By splitting up the available data (e.g. only the training data)
into seperate groups, these can be used to cross-validate the
performance of a classifier.

In sklearn one useful approach is GridSearchCV, which combines
cross-validation and parameter optimization.

Each classifer will have a range of parameters that are not
learnt when the classifer is fitted to the data. Each of
these are passed as arguments. These can have a large impact
on the performance of the classifier and fundamentaly change
how it approaches making predictions using this dataset.

Parameter optimization can be undertaken manually, running
different combinations of parameters to see which performs
best but GridSearchCV will compare combinations of classifier
parameters and see which performs the best during cross
validation.

The cross validation method can be selected, for this
use case stratified K fold is used to maintain an even
proportion of labels across the folds of data.


Testing classifiers
-------------------

Default setting
~~~~~~~~~~~~~~~

Using the default setting of one label and one feature we can take an intitial review. of the prediction.

.. code-block:: python

    features_list = ['poi', 'salary']

The outputs for the initial algorithm (Gaussian Naive Bayes) is compared to three other algorithms.

.. csv-table:: Algorith comparisson
   :header: "Algorithm", "Accuracy", "Precision", "Recall", "F1", "F2", "Tot. pred.", "True pos.", "False pos.", "False neg.", "True neg."
   :widths: 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5

   "GaussianNaiveBayes", 0.256, 0.185, 0.798, 0.300, 0.480, 10000, 1596, 7040, 404, 960
   "DecisionTree", 0,692, 0.234, 0.242, 0.239, 0.240, 10000, 483, 1562, 1517, 6438
   "RandomForest", 0.705, 0.223, 0.191, 0.205, 0.197, 10000, 382, 1328, 1618, 6672
   "AdaBoost", 0.719, 0.246, 0.196, 0.217, 0.204, 10000, 391, 1201, 1609, 6799
   "KMeans", 0.738, 0.043, 0.013, 0.020, 0.015, 370, 1, 22, 75, 272


Adaboost performs considerably slower.

KMeans gives warning about predicted labels not equal to 0 or 1.

Naive Bayes gives a very high recall value (0.798).

Gradient Boosting Classifer
~~~~~~~~~~~~~~~~~~~~~~~~~~~

After completing a version of the machine learning pipeline including
outlier removal, feature selection, feature engineering and feature scaling
a gradient boosting classifier is used with GridSearchCv. This means that
parameters can be optimized across cross-validations (in this run 2 folds
using stratified k fold). The score to optimize on is F1 weighted.

This is not removing any zeros, and using all features as input
apart from email address and those that duplicate ratio feature
engineering.

This evaluation uses a broad parameter grid.

.. code-block:: Python

    parameters = [{
                   "loss": ["deviance", "exponential"],
                   "n_estimators": [120, 300, 500, 800, 1200],
                   "max_depth": [3, 5, 7, 9, 12, 15, 17, 25],
                   "min_samples_split": [2, 5, 10, 15, 100],
                   "min_samples_leaf": [2, 5, 10],
                   "subsample": [0.6, 0.7, 0.8, 0.9, 1],
                   "max_features": ["sqrt", "log2", None]
                   }]

This gives 18000 combinations to try in an exhaustive grid search.
This is useful to get an overview of which parameter combinations
perform well, however it comes at a computational cost. It takes
a number of hours to fit the classifier. This resulted in:

Best classifier score: 0.894907227728 : 

{'subsample': 0.8, 'n_estimators': 120, 'max_depth': 25, 
'loss':'deviance', 'min_samples_split': 2, 'min_samples_leaf': 2, 
'max_features': 'sqrt'}

When applying this method using the testing function the results are:

.. csv-table:: Algorith comparisson
   :header: "Algorithm", "Accuracy", "Precision", "Recall", "F1", "F2", "Tot. pred.", "True pos.", "False pos.", "False neg.", "True neg."
   :widths: 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5

   "Gradient Boosting", 0.862, 0.454, 0.186, 0.264, 0.211, 15000, 373, 448, 1627, 12552

This method has improved on the origional methods but stil does not achieve
0.3 for precesion and recall.

The 0.45 for precesion compared to the 0.19 for recall suggests.....

Further feature optimization
~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Removing features with a high number of NaNs includes dropping,
restricted_stock_deferred, loan_advances, director_fees, deferral_payments,
and deferred_income. These variables have over 100 missing values (apart from
deferred_income with 97). The current features passing feature selection are
shown here:

['poi', 'deferred_income', 
'exercised_stock_options', 'expenses', 
'long_term_incentive', 'other', 
'restricted_stock', 'salary',
'shared_receipt_with_poi', '
total_payments', 'total_stock_value', 
'ratio_to_poi', 'ratio_from_poi']

Of these only deferred_income is currently passing through
the feature selection process. Note that bonus has also been
dropped. It is suspected that bonus is dropped as it
is correlated to a number of other variables, seen in the
pair plot during EDA.

Increasing the cut-off to 0.03 drops total_stock_value 
and shared_receipt_with_poi. This does not improve the results
using the current classifier.

The current classifer is likely overfitting the dataset
and is giving more precision than recall.

Logistic Regression
~~~~~~~~~~~~~~~~~~~

Using a cut off of 0.03.

.. csv-table:: Algorith comparisson
   :header: "Algorithm", "Accuracy", "Precision", "Recall", "F1", "F2", "Tot. pred.", "True pos.", "False pos.", "False neg.", "True neg."
   :widths: 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5

   "Logistic Regression", 0.85, 0.368, 0.177, 0.239, 0.197, 15000, 354, 609, 1646, 12391

Questions
---------


No1
~~~

Summarize for us the goal of this project and how machine learning is useful in trying to accomplish it. As part of your answer, give some background on the dataset and how it can be used to answer the project question. Were there any outliers in the data when you got it, and how did you handle those?  [relevant rubric items: “data exploration”, “outlier investigation”]


No2
~~~

What features did you end up using in your POI identifier, and what selection process did you use to pick them? Did you have to do any scaling? Why or why not? As part of the assignment, you should attempt to engineer your own feature that does not come ready-made in the dataset -- explain what feature you tried to make, and the rationale behind it. (You do not necessarily have to use it in the final analysis, only engineer and test it.) In your feature selection step, if you used an algorithm like a decision tree, please also give the feature importances of the features that you use, and if you used an automated feature selection function like SelectKBest, please report the feature scores and reasons for your choice of parameter values.  [relevant rubric items: “create new features”, “properly scale features”, “intelligently select feature”]


No3
~~~

What algorithm did you end up using? What other one(s) did you try? How did model performance differ between algorithms?  [relevant rubric item: “pick an algorithm”]


No4
~~~

What does it mean to tune the parameters of an algorithm, and what can happen if you don’t do this well?  How did you tune the parameters of your particular algorithm? What parameters did you tune? (Some algorithms do not have parameters that you need to tune -- if this is the case for the one you picked, identify and briefly explain how you would have done it for the model that was not your final choice or a different model that does utilize parameter tuning, e.g. a decision tree classifier).  [relevant rubric item: “tune the algorithm”]


No5
~~~

What is validation, and what’s a classic mistake you can make if you do it wrong? How did you validate your analysis?  [relevant rubric item: “validation strategy”]


No6
~~~

Give at least 2 evaluation metrics and your average performance for each of them.  Explain an interpretation of your metrics that says something human-understandable about your algorithm’s performance. [relevant rubric item: “usage of evaluation metrics”]


Code issues and changes
-----------------------

File Location
~~~~~~~~~~~~~

Kept getting errors about not being able to locate the file based off of the string in the original code.
Changed to:

.. code-block:: Python

    f = os.path.abspath("final_project/final_project_dataset.pkl")

Pickle
~~~~~~

Changed code in both poi_id.py and tester.py to fit with python 3 and pickle otherwise a TypeError is returned.
Now has to include "rb" (read binary) and "wb" (write binary) instead of "r" and "w" respectively.

From:

.. code-block:: Python

   with open(f, "r") as data_file:
       data_dict = pickle.load(data_file)

To:

.. code-block:: Python

    with open(f, "rb") as data_file:
        data_dict = pickle.load(data_file)


Depreciation of CV
~~~~~~~~~~~~~~~~~~

Code returns this warning.

    DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functio
    ns are moved. Also note that the interface of the new CV iterators are different from that of this module. This module w
    ill be removed in 0.20.

This has not been corrected as the starter code iterates over the cross-validation objects
and requires this.

Resources used
~~~~~~~~~~~~~~~

I hereby confirm that this submission is my work. I have cited above the origins of any parts of the submission that were taken from Websites, books, forums, blog posts, github repositories, etc.

`Sklearn API <http://scikit-learn.org/stable/modules/classes.html>`_

`Sklearn feature scaling <http://scikit-learn.org/stable/modules/preprocessing.html#preprocessing-scaler>`_

`Pandas and sklearn scaling <https://stackoverflow.com/questions/24645153/pandas-dataframe-columns-scaling-with-sklearn>`_

`Random forest parameter range suggestion <http://blog.kaggle.com/2016/07/21/approaching-almost-any-machine-learning-problem-abhishek-thakur/>`_

`Sklearn pipeline <http://scikit-learn.org/stable/modules/pipeline.html>`_

`Sklearn pipeline ANOVA feature selection <http://scikit-learn.org/stable/auto_examples/feature_selection/feature_selection_pipeline.html#sphx-glr-auto-examples-feature-selection-feature-selection-pipeline-py>`_

`Sklearn pipeline chaining PCA and logistic regression <http://scikit-learn.org/stable/auto_examples/plot_digits_pipe.html#sphx-glr-auto-examples-plot-digits-pipe-py>`_